{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias\n",
    "import numpy as np \n",
    "import numpy.typing\n",
    "\n",
    "len_seq = 1024\n",
    "n_head = 2\n",
    "d_model = 64\n",
    "d_in = 128\n",
    "d_h = d_model // n_head\n",
    "\n",
    "rng = np.random.default_rng(0x2025_07_13)\n",
    "\n",
    "Tensor32: TypeAlias = numpy.typing.NDArray[np.float32]\n",
    "\n",
    "class MultiHeadTransformer:\n",
    "    def __init__(self, d_model: int = 768, n_head: int = 12, n_layer: int = 12, seed: int | None = None):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(self.seed)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        assert d_model % n_head == 0, \"model dim must be divisible by number of heads\"\n",
    "        self.d_h = d_model // n_head\n",
    "\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "        # Key Weights\n",
    "        self.W_K: Tensor32 = self.rng.normal(size=(self.n_layer, self.d_model, self.n_head * self.d_h)).astype(np.float32)\n",
    "        # Value Weights\n",
    "        self.W_V: Tensor32 = self.rng.normal(size=(self.n_layer, self.d_model, self.n_head * self.d_h)).astype(np.float32)\n",
    "        # Query Weights\n",
    "        self.W_Q: Tensor32 = self.rng.normal(size=(self.n_layer, self.d_model, self.n_head * self.d_h)).astype(np.float32)\n",
    "        # Output Weights\n",
    "        self.W_O: Tensor32 = self.rng.normal(size=(self.n_layer, self.n_head * self.d_h, self.d_model)).astype(np.float32)\n",
    "\n",
    "    def _forward_block(self, input: Tensor32, layer: int) -> Tensor32:\n",
    "        len_seq, d_model_input = input.shape\n",
    "        assert d_model_input == self.d_model, f\"{d_model_input=}!={self.d_model=}\"\n",
    "\n",
    "        # Split off the weights of the corresponding layer\n",
    "        W_Q = self.W_Q[layer]\n",
    "        W_K = self.W_K[layer]\n",
    "        W_V = self.W_V[layer]\n",
    "        W_O = self.W_O[layer]\n",
    "\n",
    "        # Q\n",
    "        queries = np.matmul(input, W_Q).reshape(self.n_head, len_seq, self.d_h)\n",
    "        # K\n",
    "        keys = np.matmul(input, W_K).reshape(self.n_head, len_seq, self.d_h)\n",
    "        # V\n",
    "        values = np.matmul(input, W_V).reshape(self.n_head, len_seq, self.d_h)\n",
    "\n",
    "        # QK^T\n",
    "        similarities = np.matmul(queries, keys.transpose(0, 2, 1))\n",
    "        # QK^T / sqrt(d_H)\n",
    "        similarities /= np.sqrt(self.d_h).astype(np.float32)\n",
    "\n",
    "        # softmax(QK^T / sqrt(d_H))\n",
    "        attention_weights = np.exp(similarities - similarities.max(axis = -1, keepdims=True))\n",
    "        attention_weights /= attention_weights.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        # softmax(QK^T / sqrt(d_H)) * V\n",
    "        attention_output = np.matmul(attention_weights, values)\n",
    "        attention_output = attention_output.transpose(1, 0, 2).reshape(len_seq, self.n_head * self.d_h)\n",
    "\n",
    "        output = np.matmul(attention_output, W_O)\n",
    "        assert output.shape == input.shape\n",
    "        # Residual Connection\n",
    "        return input + output\n",
    "\n",
    "    def _forward(self, input: Tensor32) -> Tensor32:\n",
    "        output = input\n",
    "        for layer in range(self.n_layer):\n",
    "            output = self._forward_block(output, layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768 % 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
